{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the required libraries**"
      ],
      "metadata": {
        "id": "8FMWS6a0nnuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n"
      ],
      "metadata": {
        "id": "j-A3qalknC5o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fetch the API key**"
      ],
      "metadata": {
        "id": "E4Lc5Q8Bnqv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY:str=userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "a93ASNTf65OY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configure the Gemini Flash Model**"
      ],
      "metadata": {
        "id": "BXArxQ9qntXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7  # Adjust for creativity\n",
        ")"
      ],
      "metadata": {
        "id": "bfx_X5RpkcG1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create a Prompt Template**"
      ],
      "metadata": {
        "id": "cqEbCB_Hnx5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "73mIop23k6Sq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create a Prompt Template**"
      ],
      "metadata": {
        "id": "__GODwdZn1uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "pkpoUQLtm_iu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pass a sample question to the chain and print the response**"
      ],
      "metadata": {
        "id": "Hr_fdgdVn35g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain with a sample question\n",
        "question = \"Tell me your age?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mry_yMZmnGpp",
        "outputId": "d782345c-ad6d-46fd-9819-43d1e1185c55"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I don't have an age in the way humans do. I am a large language model, and I was created by Google. My development is ongoing, so I don't have a birthdate or a point in time when I became \"a certain age.\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langchain With Memory Feature**"
      ],
      "metadata": {
        "id": "zOeWA0jXokul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# Define a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"history\", \"question\"],\n",
        "    template=(\n",
        "        \"You are a helpful assistant. Here's the conversation so far:\\n\"\n",
        "        \"{history}\\n\\nNow answer this question:\\n{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add memory to the chain\n",
        "memory = ConversationBufferMemory(memory_key=\"history\")\n",
        "\n",
        "# Create an LLM chain with memory\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Example interaction\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"How old are you?\",\n",
        "    \"What is your favorite subject?\"\n",
        "]\n",
        "\n",
        "# Simulate a conversation\n",
        "for question in questions:\n",
        "    response = chain.run({\"question\": question})\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7aChrMroODL",
        "outputId": "fe2a03ab-3d09-48c3-bc9b-c7f412563cea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is your name?\n",
            "A: I am a large language model, trained by Google.\n",
            "\n",
            "\n",
            "Q: How old are you?\n",
            "A: I don't have an age like humans do. I was created as a computer program, not born. So, I don't have a birthday or age.\n",
            "\n",
            "\n",
            "Q: What is your favorite subject?\n",
            "A: As a large language model, I don't have personal preferences like a favorite subject. I'm designed to process and understand information across a wide range of topics. However, I do find the process of learning and generating text to be quite fascinating! Perhaps you could say my \"favorite subject\" is the vast and ever-evolving world of language and information itself.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}